{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632a5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification, EvalPrediction, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c3d86",
   "metadata": {},
   "source": [
    "### Save the path to the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20573500",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_path = \"./data_sources/train/train_en.csv\"\n",
    "test_en_path = \"./data_sources/test/test_en.csv\"\n",
    "\n",
    "train_it_path = \"./data_sources/train/train_it.csv\"\n",
    "test_it_path = \"./data_sources/test/test_it.csv\"\n",
    "\n",
    "train_es_path = \"./data_sources/train/train_es.csv\"\n",
    "test_es_path = \"./data_sources/test/test_es.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd052a",
   "metadata": {},
   "source": [
    "### Set up W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45491559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msravisconti\u001b[0m (\u001b[33msravisconti-projects\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1414d",
   "metadata": {},
   "source": [
    "### Load data in DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33d7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test CSVs\n",
    "# dataset_it = load_dataset(\"csv\", data_files={\n",
    "#     \"train\": train_it_path,\n",
    "#     \"test\": test_it_path\n",
    "# })\n",
    "\n",
    "# Load CSV manually for the train split\n",
    "train_df = pd.read_csv(train_it_path)\n",
    "test_df = pd.read_csv(test_it_path)\n",
    "\n",
    "# Split into train and val with stratification\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Convert back to Hugging Face Datasets\n",
    "dataset_it = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"val\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "383d5559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'bio', 'label', 'lang'],\n",
      "        num_rows: 694\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['id', 'text', 'bio', 'label', 'lang'],\n",
      "        num_rows: 174\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'bio', 'label', 'lang'],\n",
      "        num_rows: 218\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cf823",
   "metadata": {},
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "526d9a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 694/694 [00:00<00:00, 8017.18 examples/s]\n",
      "Map: 100%|██████████| 174/174 [00:00<00:00, 8763.00 examples/s]\n",
      "Map: 100%|██████████| 218/218 [00:00<00:00, 9096.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-base\"\n",
    "# TODO: later try with \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "\n",
    "# loads the correct tokenizer for the chosen model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# define a function to tokenize the text data\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# applies tokenize function to batches of examples to tokenize the entire dataset\n",
    "# Returns a dictionary with:\n",
    "    # input_ids → token IDs (instead of raw text)\n",
    "    # attention_mask → mask indicating which tokens are real vs padding\n",
    "    # label → original label from dataset\n",
    "tokenized_dataset_it = dataset_it.map(tokenize, batched=True)\n",
    "\n",
    "# the type of the input_ids is list\n",
    "# batch = tokenized_dataset_it[\"train\"][0]\n",
    "# print(type(batch[\"input_ids\"]))\n",
    "\n",
    "# set the format of the dataset to PyTorch tensors before feeding it to the model\n",
    "tokenized_dataset_it.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")\n",
    "\n",
    "# now the type of the input_ids is torch.Tensor\n",
    "# batch = tokenized_dataset_it[\"train\"][0]\n",
    "# print(type(batch[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd07cf",
   "metadata": {},
   "source": [
    "### Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f344feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    precision = precision_score(labels, preds, average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    # For ROC AUC, we need probabilities\n",
    "    if p.predictions.shape[1] == 2:  # binary classification\n",
    "        probs = torch.softmax(torch.tensor(p.predictions), dim=1)[:,1].numpy()\n",
    "        roc_auc = roc_auc_score(labels, probs)\n",
    "    else:\n",
    "        roc_auc = 0.0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"roc_auc\": roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14df77",
   "metadata": {},
   "source": [
    "### Define custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ac6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels distribution in training set: [562 132]\n",
      "Class weights: tensor([0.6174, 2.6288])\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights from training set\n",
    "labels = np.array(tokenized_dataset_it[\"train\"][\"label\"])\n",
    "print(\"Labels distribution in training set:\", np.bincount(labels))\n",
    "class_counts = np.bincount(labels)\n",
    "total = len(labels)\n",
    "# We want to give more weight to the class that has smaller count\n",
    "class_weights = total / (len(class_counts) * class_counts)\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class weights:\", weights_tensor)\n",
    "\n",
    "# Custom Trainer that uses weighted cross entropy\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9913282",
   "metadata": {},
   "source": [
    "### Define objective function for Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "096cf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_labels = 2  # binary classification\n",
    "batch_size = 8\n",
    "num_epochs = 4\n",
    "\n",
    "# Defines one experiment (trial)\n",
    "def objective(trial):\n",
    "    # Create a new W&B run for this trial\n",
    "    wandb.init(\n",
    "        project=\"multi-pride-bert-baseline_fixed_epochs_and_batch_size\", \n",
    "        name=f\"trial-{trial.number}\",\n",
    "        reinit=\"return_previous\"\n",
    "    )\n",
    "\n",
    "    # Sample hyperparameters for this trial\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    # batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "    # num_epochs = trial.suggest_int(\"num_epochs\", 2, 5)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "\n",
    "    # Initialize new model for this trial\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/trial-{trial.number}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",   # <-- log training loss every few steps\n",
    "        logging_steps=10,           # <-- frequency of training loss logging\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_dir=\"./logs\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"trial-{trial.number}\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\"\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset_it[\"train\"],\n",
    "        eval_dataset=tokenized_dataset_it[\"val\"],\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "\n",
    "    # ---- 8. Log both train & eval metrics to W&B ----\n",
    "    wandb.log({\n",
    "        \"train/loss\": train_result.training_loss,\n",
    "        \"eval/loss\": eval_result[\"eval_loss\"],\n",
    "        \"eval/f1\": eval_result[\"eval_f1\"],\n",
    "        \"eval/precision\": eval_result[\"eval_precision\"],\n",
    "        \"eval/recall\": eval_result[\"eval_recall\"],\n",
    "        \"eval/accuracy\": eval_result[\"eval_accuracy\"],\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"num_epochs\": num_epochs\n",
    "    })\n",
    "\n",
    "    # Finish this run cleanly\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return metric Optuna should maximize\n",
    "    return eval_result[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccff448",
   "metadata": {},
   "source": [
    "### Optuna Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 15:07:57,659] A new study created in memory with name: no-name-be6ccac4-a48c-43a2-b66f-d3908ac89198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb.init() called while a run is active and reinit is set to 'return_previous', so returning the previous run."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 32:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.255537</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.914184</td>\n",
       "      <td>0.886485</td>\n",
       "      <td>0.952934</td>\n",
       "      <td>0.965184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.187726</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.928756</td>\n",
       "      <td>0.981947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.256179</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.943907</td>\n",
       "      <td>0.943907</td>\n",
       "      <td>0.943907</td>\n",
       "      <td>0.987750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.513945</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.905589</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.977649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▆█▃██</td></tr><tr><td>eval/f1</td><td>▃▆█▁██</td></tr><tr><td>eval/loss</td><td>▂▁▂█▂▂</td></tr><tr><td>eval/precision</td><td>▁▅▆█▆▆</td></tr><tr><td>eval/recall</td><td>█▆▇▁▇▇</td></tr><tr><td>eval/roc_auc</td><td>▁▆█▅█</td></tr><tr><td>eval/runtime</td><td>▄▁▄▄█</td></tr><tr><td>eval/samples_per_second</td><td>▅█▄▅▁</td></tr><tr><td>eval/steps_per_second</td><td>▅█▄▅▁</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.96552</td></tr><tr><td>eval/f1</td><td>0.94391</td></tr><tr><td>eval/loss</td><td>0.25618</td></tr><tr><td>eval/precision</td><td>0.94391</td></tr><tr><td>eval/recall</td><td>0.94391</td></tr><tr><td>eval/roc_auc</td><td>0.98775</td></tr><tr><td>eval/runtime</td><td>23.8301</td></tr><tr><td>eval/samples_per_second</td><td>7.302</td></tr><tr><td>eval/steps_per_second</td><td>0.923</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-1</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/x036dcb0' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/x036dcb0</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_150537-x036dcb0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 15:41:16,752] Trial 0 finished with value: 0.9439071566731141 and parameters: {'learning_rate': 2.5838554043134063e-05, 'weight_decay': 0.11709120484330249}. Best is trial 0 with value: 0.9439071566731141.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_154116-8v5h269c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/8v5h269c' target=\"_blank\">trial-1</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/8v5h269c' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/8v5h269c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 34:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.256699</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.925210</td>\n",
       "      <td>0.925210</td>\n",
       "      <td>0.925210</td>\n",
       "      <td>0.988395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339400</td>\n",
       "      <td>0.265688</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.986031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.248787</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.921498</td>\n",
       "      <td>0.944828</td>\n",
       "      <td>0.901999</td>\n",
       "      <td>0.986245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.384554</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.983666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁█▁▅██</td></tr><tr><td>eval/f1</td><td>▂█▁▄██</td></tr><tr><td>eval/loss</td><td>▁▂▁█▂▂</td></tr><tr><td>eval/precision</td><td>▁█▄███</td></tr><tr><td>eval/recall</td><td>█▄▃▁▄▄</td></tr><tr><td>eval/roc_auc</td><td>█▅▅▁▅</td></tr><tr><td>eval/runtime</td><td>█▅▁▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▄█▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▄█▆▇</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.96552</td></tr><tr><td>eval/f1</td><td>0.93958</td></tr><tr><td>eval/loss</td><td>0.26569</td></tr><tr><td>eval/precision</td><td>0.97959</td></tr><tr><td>eval/recall</td><td>0.90909</td></tr><tr><td>eval/roc_auc</td><td>0.98603</td></tr><tr><td>eval/runtime</td><td>24.4797</td></tr><tr><td>eval/samples_per_second</td><td>7.108</td></tr><tr><td>eval/steps_per_second</td><td>0.899</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-1</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/8v5h269c' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/8v5h269c</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_154116-8v5h269c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 16:16:07,413] Trial 1 finished with value: 0.9395833333333333 and parameters: {'learning_rate': 1.0592850462189524e-05, 'weight_decay': 0.05603714798343684}. Best is trial 0 with value: 0.9439071566731141.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_161607-h6ja1awg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/h6ja1awg' target=\"_blank\">trial-2</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/h6ja1awg' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/h6ja1awg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 32:52, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.415300</td>\n",
       "      <td>0.332366</td>\n",
       "      <td>0.867816</td>\n",
       "      <td>0.820980</td>\n",
       "      <td>0.789880</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>0.897271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.614231</td>\n",
       "      <td>0.936782</td>\n",
       "      <td>0.884608</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>0.844939</td>\n",
       "      <td>0.906727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.258583</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.935306</td>\n",
       "      <td>0.930462</td>\n",
       "      <td>0.940361</td>\n",
       "      <td>0.985171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.414161</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.899306</td>\n",
       "      <td>0.935752</td>\n",
       "      <td>0.871696</td>\n",
       "      <td>0.985171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▆█▇██</td></tr><tr><td>eval/f1</td><td>▁▅█▆██</td></tr><tr><td>eval/loss</td><td>▂█▁▄▁▁</td></tr><tr><td>eval/precision</td><td>▁█▇█▇▇</td></tr><tr><td>eval/recall</td><td>▅▁█▃██</td></tr><tr><td>eval/roc_auc</td><td>▁▂███</td></tr><tr><td>eval/runtime</td><td>▃▂█▁▇</td></tr><tr><td>eval/samples_per_second</td><td>▅▇▁█▂</td></tr><tr><td>eval/steps_per_second</td><td>▆▇▁█▂</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.95977</td></tr><tr><td>eval/f1</td><td>0.93531</td></tr><tr><td>eval/loss</td><td>0.25858</td></tr><tr><td>eval/precision</td><td>0.93046</td></tr><tr><td>eval/recall</td><td>0.94036</td></tr><tr><td>eval/roc_auc</td><td>0.98517</td></tr><tr><td>eval/runtime</td><td>23.7765</td></tr><tr><td>eval/samples_per_second</td><td>7.318</td></tr><tr><td>eval/steps_per_second</td><td>0.925</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-2</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/h6ja1awg' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/h6ja1awg</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_161607-h6ja1awg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 16:49:38,013] Trial 2 finished with value: 0.9353056780156159 and parameters: {'learning_rate': 2.657845678465721e-05, 'weight_decay': 0.006787039205170075}. Best is trial 0 with value: 0.9439071566731141.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_164938-xi6xnbef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/xi6xnbef' target=\"_blank\">trial-3</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/xi6xnbef' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/xi6xnbef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 32:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.201274</td>\n",
       "      <td>0.936782</td>\n",
       "      <td>0.906520</td>\n",
       "      <td>0.877165</td>\n",
       "      <td>0.949387</td>\n",
       "      <td>0.983666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.442000</td>\n",
       "      <td>0.306738</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.984096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.223248</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.928756</td>\n",
       "      <td>0.985386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.392280</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.984956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁█████</td></tr><tr><td>eval/f1</td><td>▁▇█▇██</td></tr><tr><td>eval/loss</td><td>▁▅▂█▂▂</td></tr><tr><td>eval/precision</td><td>▁█▅█▅▅</td></tr><tr><td>eval/recall</td><td>█▁▅▁▅▅</td></tr><tr><td>eval/roc_auc</td><td>▁▃█▆█</td></tr><tr><td>eval/runtime</td><td>██▅▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▄▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▄▄█</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.95977</td></tr><tr><td>eval/f1</td><td>0.93379</td></tr><tr><td>eval/loss</td><td>0.22325</td></tr><tr><td>eval/precision</td><td>0.93904</td></tr><tr><td>eval/recall</td><td>0.92876</td></tr><tr><td>eval/roc_auc</td><td>0.98539</td></tr><tr><td>eval/runtime</td><td>21.0745</td></tr><tr><td>eval/samples_per_second</td><td>8.256</td></tr><tr><td>eval/steps_per_second</td><td>1.044</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-3</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/xi6xnbef' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/xi6xnbef</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_164938-xi6xnbef\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 17:22:46,072] Trial 3 finished with value: 0.9337863549877685 and parameters: {'learning_rate': 1.1552318327983594e-05, 'weight_decay': 0.03813173093123977}. Best is trial 0 with value: 0.9439071566731141.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_172246-jcf421j5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/jcf421j5' target=\"_blank\">trial-4</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/jcf421j5' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/jcf421j5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 31:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.518200</td>\n",
       "      <td>0.227777</td>\n",
       "      <td>0.971264</td>\n",
       "      <td>0.953790</td>\n",
       "      <td>0.948739</td>\n",
       "      <td>0.959059</td>\n",
       "      <td>0.987105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.278615</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.209730</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.928756</td>\n",
       "      <td>0.988395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.349010</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.982377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>█▄▁▁██</td></tr><tr><td>eval/f1</td><td>█▄▂▁██</td></tr><tr><td>eval/loss</td><td>▂▄▁█▂▂</td></tr><tr><td>eval/precision</td><td>▃█▁▇▃▃</td></tr><tr><td>eval/recall</td><td>█▃▅▁██</td></tr><tr><td>eval/roc_auc</td><td>▇▁█▃▇</td></tr><tr><td>eval/runtime</td><td>▁▄▆▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▃▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▃▃▁</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.97126</td></tr><tr><td>eval/f1</td><td>0.95379</td></tr><tr><td>eval/loss</td><td>0.22778</td></tr><tr><td>eval/precision</td><td>0.94874</td></tr><tr><td>eval/recall</td><td>0.95906</td></tr><tr><td>eval/roc_auc</td><td>0.98711</td></tr><tr><td>eval/runtime</td><td>23.7604</td></tr><tr><td>eval/samples_per_second</td><td>7.323</td></tr><tr><td>eval/steps_per_second</td><td>0.926</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-4</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/jcf421j5' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/jcf421j5</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_172246-jcf421j5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 17:55:00,039] Trial 4 finished with value: 0.9537897700111542 and parameters: {'learning_rate': 1.0327292968111504e-05, 'weight_decay': 0.06144150815589746}. Best is trial 4 with value: 0.9537897700111542.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_175500-itlp4zdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/itlp4zdb' target=\"_blank\">trial-5</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/itlp4zdb' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/itlp4zdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 33:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.308886</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.845553</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.929078</td>\n",
       "      <td>0.910380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.265609</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.942562</td>\n",
       "      <td>0.953756</td>\n",
       "      <td>0.932302</td>\n",
       "      <td>0.973995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.295609</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.928756</td>\n",
       "      <td>0.987965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.344342</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.933786</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.928756</td>\n",
       "      <td>0.987750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁█████</td></tr><tr><td>eval/f1</td><td>▁█▇▇██</td></tr><tr><td>eval/loss</td><td>▅▁▄█▁▁</td></tr><tr><td>eval/precision</td><td>▁█▇▇██</td></tr><tr><td>eval/recall</td><td>▂█▁▁██</td></tr><tr><td>eval/roc_auc</td><td>▁▇██▇</td></tr><tr><td>eval/runtime</td><td>▆▁▄▂█</td></tr><tr><td>eval/samples_per_second</td><td>▃█▅▇▁</td></tr><tr><td>eval/steps_per_second</td><td>▃█▅▇▁</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.96552</td></tr><tr><td>eval/f1</td><td>0.94256</td></tr><tr><td>eval/loss</td><td>0.26561</td></tr><tr><td>eval/precision</td><td>0.95376</td></tr><tr><td>eval/recall</td><td>0.9323</td></tr><tr><td>eval/roc_auc</td><td>0.974</td></tr><tr><td>eval/runtime</td><td>25.7079</td></tr><tr><td>eval/samples_per_second</td><td>6.768</td></tr><tr><td>eval/steps_per_second</td><td>0.856</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-5</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/itlp4zdb' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/itlp4zdb</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_175500-itlp4zdb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 18:29:13,295] Trial 5 finished with value: 0.9425616197183099 and parameters: {'learning_rate': 3.419249276404569e-05, 'weight_decay': 0.12480186858696667}. Best is trial 4 with value: 0.9537897700111542.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_182913-l8l30v3u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/l8l30v3u' target=\"_blank\">trial-6</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/l8l30v3u' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/l8l30v3u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 33:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.211992</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.884330</td>\n",
       "      <td>0.851680</td>\n",
       "      <td>0.938749</td>\n",
       "      <td>0.961530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.533300</td>\n",
       "      <td>0.398391</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.942833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.334590</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.941123</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.920696</td>\n",
       "      <td>0.970557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.417013</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.917241</td>\n",
       "      <td>0.973154</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.981732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▆█▆██</td></tr><tr><td>eval/f1</td><td>▁▅█▅██</td></tr><tr><td>eval/loss</td><td>▁▇▅█▅▅</td></tr><tr><td>eval/precision</td><td>▁█████</td></tr><tr><td>eval/recall</td><td>█▁▆▁▆▆</td></tr><tr><td>eval/roc_auc</td><td>▄▁▆█▆</td></tr><tr><td>eval/runtime</td><td>▁▁▃▇█</td></tr><tr><td>eval/samples_per_second</td><td>██▆▂▁</td></tr><tr><td>eval/steps_per_second</td><td>██▆▂▁</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.96552</td></tr><tr><td>eval/f1</td><td>0.94112</td></tr><tr><td>eval/loss</td><td>0.33459</td></tr><tr><td>eval/precision</td><td>0.96552</td></tr><tr><td>eval/recall</td><td>0.9207</td></tr><tr><td>eval/roc_auc</td><td>0.97056</td></tr><tr><td>eval/runtime</td><td>24.8679</td></tr><tr><td>eval/samples_per_second</td><td>6.997</td></tr><tr><td>eval/steps_per_second</td><td>0.885</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-6</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/l8l30v3u' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/l8l30v3u</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_182913-l8l30v3u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 19:03:38,578] Trial 6 finished with value: 0.941123392736296 and parameters: {'learning_rate': 1.2595311189837818e-05, 'weight_decay': 0.08732439085204093}. Best is trial 4 with value: 0.9537897700111542.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_190338-mvk0xwpu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/mvk0xwpu' target=\"_blank\">trial-7</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/mvk0xwpu' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/mvk0xwpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 34:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.890805</td>\n",
       "      <td>0.849693</td>\n",
       "      <td>0.815968</td>\n",
       "      <td>0.921019</td>\n",
       "      <td>0.985386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.305873</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.914868</td>\n",
       "      <td>0.919894</td>\n",
       "      <td>0.910058</td>\n",
       "      <td>0.980658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.365993</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.921498</td>\n",
       "      <td>0.944828</td>\n",
       "      <td>0.901999</td>\n",
       "      <td>0.984741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.413705</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>0.899306</td>\n",
       "      <td>0.935752</td>\n",
       "      <td>0.871696</td>\n",
       "      <td>0.983022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▇█▇██</td></tr><tr><td>eval/f1</td><td>▁▇█▆██</td></tr><tr><td>eval/loss</td><td>▁▂▅█▅▅</td></tr><tr><td>eval/precision</td><td>▁▇████</td></tr><tr><td>eval/recall</td><td>█▆▅▁▅▅</td></tr><tr><td>eval/roc_auc</td><td>█▁▇▅▇</td></tr><tr><td>eval/runtime</td><td>▅▆█▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▄▃▁▄█</td></tr><tr><td>eval/steps_per_second</td><td>▄▃▁▄█</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>8</td></tr><tr><td>eval/accuracy</td><td>0.95402</td></tr><tr><td>eval/f1</td><td>0.9215</td></tr><tr><td>eval/loss</td><td>0.36599</td></tr><tr><td>eval/precision</td><td>0.94483</td></tr><tr><td>eval/recall</td><td>0.902</td></tr><tr><td>eval/roc_auc</td><td>0.98474</td></tr><tr><td>eval/runtime</td><td>22.6816</td></tr><tr><td>eval/samples_per_second</td><td>7.671</td></tr><tr><td>eval/steps_per_second</td><td>0.97</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial-7</strong> at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/mvk0xwpu' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/mvk0xwpu</a><br> View project at: <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251111_190338-mvk0xwpu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 19:38:53,253] Trial 7 finished with value: 0.9214978569817279 and parameters: {'learning_rate': 2.5134362640363844e-05, 'weight_decay': 0.2588509643495115}. Best is trial 4 with value: 0.9537897700111542.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251111_193853-zt29sch0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/zt29sch0' target=\"_blank\">trial-8</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/zt29sch0' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/zt29sch0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='349' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 33:52, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.191139</td>\n",
       "      <td>0.936782</td>\n",
       "      <td>0.906520</td>\n",
       "      <td>0.877165</td>\n",
       "      <td>0.949387</td>\n",
       "      <td>0.966043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.337532</td>\n",
       "      <td>0.959770</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.984956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.381831</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.990974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.407513</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.890393</td>\n",
       "      <td>0.987965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "[W 2025-11-11 20:13:29,538] Trial 8 failed with parameters: {'learning_rate': 1.9651715790283235e-05, 'weight_decay': 0.10665126050623909} because of the following error: RuntimeError('[enforce fail at inline_container.cc:664] . unexpected pos 46720 vs 46612').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 967, in save\n",
      "    _save(\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 1268, in _save\n",
      "    zip_file.write_record(name, storage, num_bytes)\n",
      "RuntimeError: [enforce fail at inline_container.cc:863] . PytorchStreamWriter failed writing file data/1: file write failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\sravi\\AppData\\Local\\Temp\\ipykernel_24560\\2935411231.py\", line 51, in objective\n",
      "    train_result = trainer.train()\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 2328, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 2788, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 3234, in _maybe_log_save_evaluate\n",
      "    self._save_checkpoint(model, trial)\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 3342, in _save_checkpoint\n",
      "    self._save_optimizer_and_scheduler(output_dir)\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 3469, in _save_optimizer_and_scheduler\n",
      "    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 966, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py\", line 798, in __exit__\n",
      "    self.file_like.write_end_of_file()\n",
      "RuntimeError: [enforce fail at inline_container.cc:664] . unexpected pos 46720 vs 46612\n",
      "[W 2025-11-11 20:13:29,889] Trial 8 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:664] . unexpected pos 46720 vs 46612",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py:967\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py:1268\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1267\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:863] . PytorchStreamWriter failed writing file data/1: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(study.best_trial.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     25\u001b[39m training_args = TrainingArguments(\n\u001b[32m     26\u001b[39m     output_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./results/trial-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial.number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     metric_for_best_model=\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m trainer = WeightedTrainer(\n\u001b[32m     44\u001b[39m     model=model,\n\u001b[32m     45\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     49\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m eval_result = trainer.evaluate()\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ---- 8. Log both train & eval metrics to W&B ----\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2788\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2785\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2787\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2790\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2793\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2794\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3234\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3231\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3234\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3235\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3342\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3338\u001b[39m         \u001b[38;5;28mself\u001b[39m.state.best_model_checkpoint = best_checkpoint_dir\n\u001b[32m   3340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3341\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3342\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3343\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3344\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3469\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3464\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3465\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3466\u001b[39m     )\n\u001b[32m   3467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3468\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3471\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3472\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3473\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3474\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py:966\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m     f = os.fspath(f)\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    967\u001b[39m         _save(\n\u001b[32m    968\u001b[39m             obj,\n\u001b[32m    969\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    972\u001b[39m             _disable_byteorder_record,\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\serialization.py:798\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__exit__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.file_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    800\u001b[39m         \u001b[38;5;28mself\u001b[39m.file_stream.close()\n",
      "\u001b[31mRuntimeError\u001b[39m: [enforce fail at inline_container.cc:664] . unexpected pos 46720 vs 46612"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5)  \n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc594ad0",
   "metadata": {},
   "source": [
    "### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53310ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 868/868 [00:00<00:00, 11521.39 examples/s]\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\wandb\\run-20251117_183611-4s3hpizb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/4s3hpizb' target=\"_blank\">final model</a></strong> to <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/4s3hpizb' target=\"_blank\">https://wandb.ai/sravisconti-projects/multi-pride-bert-baseline_fixed_epochs_and_batch_size/runs/4s3hpizb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='436' max='436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [436/436 41:21, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.390409</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.867289</td>\n",
       "      <td>0.867289</td>\n",
       "      <td>0.867289</td>\n",
       "      <td>0.950758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.283283</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.894860</td>\n",
       "      <td>0.902528</td>\n",
       "      <td>0.887716</td>\n",
       "      <td>0.966450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.180300</td>\n",
       "      <td>0.431339</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.894860</td>\n",
       "      <td>0.902528</td>\n",
       "      <td>0.887716</td>\n",
       "      <td>0.963609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.481175</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.894860</td>\n",
       "      <td>0.902528</td>\n",
       "      <td>0.887716</td>\n",
       "      <td>0.966315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sravi\\Desktop\\Projects\\multi-pride\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test metrics: {'eval_loss': 0.2832830548286438, 'eval_accuracy': 0.9357798165137615, 'eval_f1': 0.8948601350420284, 'eval_precision': 0.9025280898876404, 'eval_recall': 0.8877164502164503, 'eval_roc_auc': 0.9664502164502164, 'eval_runtime': 28.9454, 'eval_samples_per_second': 7.531, 'eval_steps_per_second': 0.967, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Merge train + val for final training\n",
    "final_train_df = pd.concat([train_df, val_df])\n",
    "final_train_dataset = Dataset.from_pandas(final_train_df)\n",
    "final_train_dataset = final_train_dataset.map(tokenize, batched=True)\n",
    "final_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load best hyperparameters\n",
    "# best_params = study.best_trial.params\n",
    "best_params = {\n",
    "    \"learning_rate\": 1.0327292968111504e-05,\n",
    "    \"weight_decay\": 0.06144150815589746\n",
    "}\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"./final_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1, \n",
    "    logging_strategy=\"steps\",   # <-- log training loss every few steps\n",
    "    logging_steps=10,           # <-- frequency of training loss logging\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"final-model\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "final_trainer = WeightedTrainer(\n",
    "    model=final_model,\n",
    "    args=final_args,\n",
    "    train_dataset=final_train_dataset,\n",
    "    eval_dataset=tokenized_dataset_it[\"test\"],  # final test only once!\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "        project=\"multi-pride-bert-baseline_fixed_epochs_and_batch_size\", \n",
    "        name=f\"final model\",\n",
    "        reinit=\"return_previous\",\n",
    "        resume=\"allow\"\n",
    "    )\n",
    "\n",
    "final_trainer.train()\n",
    "final_results = final_trainer.evaluate()\n",
    "wandb.log(final_results)\n",
    "print(\"Final test metrics:\", final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-pride",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
